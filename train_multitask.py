import os
import datetime
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup
from torch.optim import AdamW
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, accuracy_score
from tqdm.auto import tqdm

# === –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ===
BASE_MODEL = "DeepPavlov/rubert-base-cased"  # –ë–∞–∑–æ–≤–∞—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å BERT
SAVE_ROOT = "models"                         # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
BATCH_SIZE = 4                               # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
EPOCHS = 5                                   # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
MAX_LEN = 100                                # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)
LR = 2e-5                                    # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
os.makedirs(SAVE_ROOT, exist_ok=True)        # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç

print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")

# === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
data = pd.read_csv("data/incidents_fuul.csv")   # –ó–∞–≥—Ä—É–∂–∞–µ–º CSV —Å –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞–º–∏
data = data.dropna(subset=["urgency"])          # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –º–µ—Ç–∫–∏ —Å—Ä–æ—á–Ω–æ—Å—Ç–∏
print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π")

# === –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
print("–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫...")
le_category = LabelEncoder()
data["category_label"] = le_category.fit_transform(data["category"])  # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ ‚Üí —á–∏—Å–ª–∞
le_urgency = LabelEncoder()
data["urgency_label"] = le_urgency.fit_transform(data["urgency"])     # –°—Ä–æ—á–Ω–æ—Å—Ç—å ‚Üí —á–∏—Å–ª–∞

# –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –≤ —Ç–µ–∫—Å—Ç (data augmentation)
data["text_aug"] = "[–∫–∞—Ç–µ–≥–æ—Ä–∏—è: " + data["category"] + "] " + data["text"]

# === –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/val/test ===
print("–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
train_texts, temp_texts, train_cat, temp_cat, train_urg, temp_urg = train_test_split(
    data["text_aug"], data["category_label"], data["urgency_label"], 
    test_size=0.3, random_state=42, stratify=data["urgency_label"]
)
val_texts, test_texts, val_cat, test_cat, val_urg, test_urg = train_test_split(
    temp_texts, temp_cat, temp_urg, test_size=0.6667, random_state=42, stratify=temp_urg
)
print(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö: Train={len(train_texts)}, Val={len(val_texts)}, Test={len(test_texts)}")

# === –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ===
print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)

# === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞ Dataset ===
class IncidentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
            return_attention_mask=True,
            return_tensors="pt"
        )
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Ç–µ–Ω–∑–æ—Ä–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
        return {
            "input_ids": encoding["input_ids"].flatten(),
            "attention_mask": encoding["attention_mask"].flatten(),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long)
        }

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è DataLoader
def create_loader(texts, labels, shuffle=True):
    ds = IncidentDataset(texts.tolist(), labels.tolist(), tokenizer, MAX_LEN)
    return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=shuffle)

# === –°–æ–∑–¥–∞–Ω–∏–µ DataLoader –¥–ª—è –æ–±–µ–∏—Ö –∑–∞–¥–∞—á ===
print("–°–æ–∑–¥–∞–Ω–∏–µ DataLoader...")
train_cat_loader = create_loader(train_texts, train_cat)
val_cat_loader = create_loader(val_texts, val_cat, shuffle=False)
test_cat_loader = create_loader(test_texts, test_cat, shuffle=False)

train_urg_loader = create_loader(train_texts, train_urg)
val_urg_loader = create_loader(val_texts, val_urg, shuffle=False)
test_urg_loader = create_loader(test_texts, test_urg, shuffle=False)

# === –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ ===
num_cat = len(le_category.classes_)
num_urg = len(le_urgency.classes_)
print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–π: {num_cat}, –°—Ä–æ—á–Ω–æ—Å—Ç—å: {num_urg}")

# === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π BERT –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ ===
print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
model_cat = BertForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=num_cat).to(DEVICE)
model_urg = BertForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=num_urg).to(DEVICE)

# === –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å—Ä–æ—á–Ω–æ—Å—Ç–∏ ===
print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤...")
class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.arange(num_urg),
    y=train_urg
)
class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)
loss_fn_urg = torch.nn.CrossEntropyLoss(weight=class_weights)  # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å

# === –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –∏ —à–µ–¥—É–ª–µ—Ä—ã ===
optimizer_cat = AdamW(model_cat.parameters(), lr=LR)
optimizer_urg = AdamW(model_urg.parameters(), lr=LR)

scheduler_cat = get_linear_schedule_with_warmup(
    optimizer_cat, num_warmup_steps=0, num_training_steps=len(train_cat_loader)*EPOCHS
)
scheduler_urg = get_linear_schedule_with_warmup(
    optimizer_urg, num_warmup_steps=0, num_training_steps=len(train_urg_loader)*EPOCHS
)

# === –§—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ ===
def eval_model(model, loader, desc="Evaluation"):
    model.eval()
    preds, labels = [], []
    with torch.no_grad():
        for batch in tqdm(loader, desc=desc, leave=False):
            input_ids = batch["input_ids"].to(DEVICE)
            attention_mask = batch["attention_mask"].to(DEVICE)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
            labels.extend(batch["labels"].numpy())
    acc = accuracy_score(labels, preds)
    return acc, labels, preds

# === –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ ===
def train_epoch(model, optimizer, scheduler, loader, loss_fn=None, desc="Training"):
    model.train()
    total_loss = 0
    progress_bar = tqdm(loader, desc=desc, leave=False)
    for batch in progress_bar:
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(DEVICE)
        attention_mask = batch["attention_mask"].to(DEVICE)
        labels = batch["labels"].to(DEVICE)
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
        if loss_fn is None:
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
        else:
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = loss_fn(outputs.logits, labels)
        # –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        loss.backward()
        optimizer.step()
        scheduler.step()
        total_loss += loss.item()
        progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})
    return total_loss / len(loader)

# === –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è ===
print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
history = {'epoch': [], 'cat_loss': [], 'urg_loss': [], 'val_cat_acc': [], 'val_urg_acc': []}

for epoch in range(EPOCHS):
    print(f"\n{'='*50}")
    print(f"–≠–ü–û–•–ê {epoch+1}/{EPOCHS}")
    print(f"{'='*50}")
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    avg_cat_loss = train_epoch(
        model_cat, optimizer_cat, scheduler_cat, train_cat_loader, desc=f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ [–≠–ø–æ—Ö–∞ {epoch+1}]"
    )
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–æ—á–Ω–æ—Å—Ç–∏ (—Å –≤–µ—Å–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤)
    avg_urg_loss = train_epoch(
        model_urg, optimizer_urg, scheduler_urg, train_urg_loader, loss_fn_urg, desc=f"–°—Ä–æ—á–Ω–æ—Å—Ç—å [–≠–ø–æ—Ö–∞ {epoch+1}]"
    )
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    cat_acc, cat_labels, cat_preds = eval_model(model_cat, val_cat_loader, desc=f"–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
    urg_acc, urg_labels, urg_preds = eval_model(model_urg, val_urg_loader, desc=f"–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ä–æ—á–Ω–æ—Å—Ç–∏")
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    history['epoch'].append(epoch + 1)
    history['cat_loss'].append(avg_cat_loss)
    history['urg_loss'].append(avg_urg_loss)
    history['val_cat_acc'].append(cat_acc)
    history['val_urg_acc'].append(urg_acc)
    print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∏: loss={avg_cat_loss:.4f}, val_acc={cat_acc:.3f}")
    print(f"–°—Ä–æ—á–Ω–æ—Å—Ç—å: loss={avg_urg_loss:.4f}, val_acc={urg_acc:.3f}")

# === –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ===
print(f"\n{'='*50}")
print("–§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
print(f"{'='*50}")
cat_acc, cat_labels, cat_preds = eval_model(model_cat, test_cat_loader, desc="üìä –¢–µ—Å—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
urg_acc, urg_labels, urg_preds = eval_model(model_urg, test_urg_loader, desc="üìä –¢–µ—Å—Ç —Å—Ä–æ—á–Ω–æ—Å—Ç–∏")
print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (—Ç–µ—Å—Ç): acc={cat_acc:.3f}")
print(f"–°—Ä–æ—á–Ω–æ—Å—Ç—å (—Ç–µ—Å—Ç): acc={urg_acc:.3f}")

# === –û—Ç—á—ë—Ç—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É ===
print(f"\n{'='*50}")
print("–ò–¢–û–ì–û–í–´–ï –û–¢–ß–Å–¢–´")
print(f"{'='*50}")
print("\n=== –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ ===")
print(classification_report(cat_labels, cat_preds, target_names=le_category.classes_))
print("\n=== –°—Ä–æ—á–Ω–æ—Å—Ç—å ===")
print(classification_report(urg_labels, urg_preds, target_names=le_urgency.classes_))

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
save_dir = f"{SAVE_ROOT}/dual_rubert_{ts}"
os.makedirs(save_dir, exist_ok=True)
print(f"\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤: {save_dir}")

try:
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
    print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π...")
    model_cat.save_pretrained(f"{save_dir}/category_model")
    print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å—Ä–æ—á–Ω–æ—Å—Ç–∏...")
    model_urg.save_pretrained(f"{save_dir}/urgency_model")
    print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer.save_pretrained(save_dir)
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ LabelEncoder
    print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤...")
    pd.to_pickle(le_category, f"{save_dir}/le_category.pkl")
    pd.to_pickle(le_urgency, f"{save_dir}/le_urgency.pkl")
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
    print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è...")
    history_df = pd.DataFrame(history)
    history_df.to_csv(f"{save_dir}/training_history.csv", index=False)
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–± –æ–±—É—á–µ–Ω–∏–∏
    print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
    training_info = {
        'base_model': BASE_MODEL,
        'batch_size': BATCH_SIZE,
        'epochs': EPOCHS,
        'max_len': MAX_LEN,
        'learning_rate': LR,
        'device': str(DEVICE),
        'timestamp': ts,
        'test_accuracy_category': cat_acc,
        'test_accuracy_urgency': urg_acc,
        'num_categories': num_cat,
        'num_urgency_levels': num_urg
    }
    info_df = pd.DataFrame([training_info])
    info_df.to_csv(f"{save_dir}/training_info.csv", index=False)
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"–í—Å–µ —Ñ–∞–π–ª—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {save_dir}")
    print(f"–ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"   ‚Ä¢ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {cat_acc:.3f}")
    print(f"   ‚Ä¢ –°—Ä–æ—á–Ω–æ—Å—Ç—å: {urg_acc:.3f}")
    # –í—ã–≤–æ–¥ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–∞–ø–∫–∏
    print(f"\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞–ø–∫–∏ {save_dir}:")
    for file in os.listdir(save_dir):
        file_path = os.path.join(save_dir, file)
        if os.path.isdir(file_path):
            print(f"   {file}/")
        else:
            size = os.path.getsize(file_path)
            print(f"    {file} ({size} bytes)")
except Exception as e:
    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
